<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>04 Word Frequency and Keywords</title>
    <meta charset="utf-8" />
    <meta name="author" content="Lars Hinrichs Digital Text Analysis" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link href="index_files/tile-view/tile-view.css" rel="stylesheet" />
    <script src="index_files/tile-view/tile-view.js"></script>
    <link href="index_files/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link href="index_files/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="index_files/panelset/panelset.css" rel="stylesheet" />
    <script src="index_files/panelset/panelset.js"></script>
    <script src="index_files/fabric/fabric.min.js"></script>
    <link href="index_files/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="index_files/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <script src="index_files/kePrint/kePrint.js"></script>
    <link href="index_files/lightable/lightable.css" rel="stylesheet" />
    <script src="index_files/htmlwidgets/htmlwidgets.js"></script>
    <link href="index_files/wordcloud2/wordcloud.css" rel="stylesheet" />
    <script src="index_files/wordcloud2/wordcloud2-all.js"></script>
    <script src="index_files/wordcloud2/hover.js"></script>
    <script src="index_files/wordcloud2-binding/wordcloud2.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GV89SH3H9Y"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-GV89SH3H9Y');
      </script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 04 Word Frequency and Keywords
### Lars Hinrichs<br />Digital Text Analysis

---

class: center, middle










## Outline

Word frequencies&lt;br /&gt;
Stopwords&lt;br /&gt;
Word clouds&lt;br /&gt;
Keywords

---
class: inverse, center, middle

# Word frequencies

---
class: middle

## Word frequencies

Basic assumption: more frequent words are more "important"; more representative of a corpus's style/content/cognitive style

---
class:middle

## Get the top-20 most frequent words in your corpus



First, tokenize your corpus using unnest_tokens() from {tidytext}.


```r
corpus_tokens &lt;- 
  corpus %&gt;% 
  unnest_tokens(word, text)
```

---



## Count word forms

Now, use count() to get the frequencies of unique word forms.

.panelset[
.panel[.panel-name[Code]


```r
top20 &lt;- 
  corpus_tokens %&gt;% 
  count(word, sort = T) %&gt;% 
  slice_max(n, n=20)

top20 
```

]

.panel[.panel-name[Output]
&lt;img src="top20.png" width="30%" /&gt;
]
]

---

## Observation

Practically all words in the top-20 are function words (i.e. grammatical words), with two exceptions: *said* and *have*.

--

But that finding is nearly useless: any other corpus of English would look pretty much identical!

---
class: middle

.pull-left[
This is where it makes sense to remove **stop words** from the corpus: a predefined list of high-frequency function words. {tidytext}  contains a tibble of stop words that we can use for this purpose. It is called stop_words.]

.pull-right[


```
## # A tibble: 1,149 × 2
##    word        lexicon
##    &lt;chr&gt;       &lt;chr&gt;  
##  1 a           SMART  
##  2 a's         SMART  
##  3 able        SMART  
##  4 about       SMART  
##  5 above       SMART  
##  6 according   SMART  
##  7 accordingly SMART  
##  8 across      SMART  
##  9 actually    SMART  
## 10 after       SMART  
## # … with 1,139 more rows
```
]

---

## Removing stop words from a corpus

Because stop_words has a column with the same name as one of the columns in our corpus--the column **words** - we can easily remove all stop words using anti_join(). 

&gt; Please read up on the *join* group of commands -  anti_join(), left_join(), etc. -  [here](https://dplyr.tidyverse.org/reference/mutate-joins.html) and [here](https://dplyr.tidyverse.org/reference/filter-joins.html).


```r
corpus_tokens &lt;- 
  corpus_tokens %&gt;% 
  anti_join(stop_words)
```

---
class:middle

By dropping the stop words in our corpus, we've just lost a little more than half of our corpus size, but that's okay. At least we now know that the remaining words are actually interesting! 

Let's run the top-20 analysis again.

---

## Top-20 after stopword removal

.panelset[
.panel[.panel-name[Code]


```r
top20 &lt;- 
  corpus_tokens %&gt;% 
  count(word, sort = T) %&gt;% 
  slice_max(n, n=20)

top20 
```

]

.panel[.panel-name[Output]
&lt;img src="top20_2_result.png" width="30%" /&gt;
]
]

---

## Observation

.pull-left[
Much better! This gives us an idea of what's going on in our corpus - at least of what some of the dominant topics are.

But still not perfect. What else remains in the list that is not helpful? 
]

.pull-right[
&lt;img src="top20_2_result.png" width="90%" /&gt;

]


---
class: middle, center


&lt;img src="top20_2_result.png" width="60%" /&gt;

---

## Plan of action

We should

- write our own custom list of words to exclude in addition to the stop words (e.g. *it's*, which should have been dropped via stop word removal, but wasn't),
- find a way to handle different inflected forms of the same word (e.g. *time/times*),
- think about whether we want to conflate pairs of synonyms (e.g. *doctor* and *doc*).

---

## Write a custom "blacklist" of words 

We'll write a simple vector of words we want to exclude. Since a vector is not a tibble (or data frame), we can't use anti_join() to remove those words from the corpus_tokens object. But we can use filter() to achieve the same thing.


```r
my_excl &lt;- 
  c("it’s", "time")

corpus_tokens &lt;- 
  corpus_tokens %&gt;% 
  filter(!word %in% my_excl)
```

---

## Dealing with typographic punctuation

The reason why we still had "it's" in the corpus (even after stop word removal) is because there were typographic apostrophes in there. And the stop_words list only knows straight apostrophes. 

Let us replace one with the other, i.e. all typographic apostrophes will be replaced by straight ones: out with ’ and in with '. **We'll use another function from {stringr} for this: str_replace_all().**


```r
corpus_tokens &lt;- 
  corpus_tokens %&gt;% 
  mutate(word = str_replace_all(word, "’", "'"))
```

---

## Different forms of the same word

e.g. *time/times*

possible remedies: stemming vs. lemmatizing

Either one of the procedures above would help here, but lemmatization is clearly better. We will discuss it in an upcoming class session when we talk about *annotation*.

---
class:middle

## Synonyms

If it is important to the analysis, then we may sometimes want to map several synonyms onto one term. Simply for practice, let's say we want to consolidate variation among different terms for the current pandemic: in our corpus we encounter *coronavirus, covid*, and *pandemic*, but let's pretend we want them all to be replaced by one term, *covid-19.* 

This will give us one term in the top-20 list with a much higher hit number than if we kept the three terms separate.

---
class:middle

Using the str_replace_all() function, here is how to search for several options and replace with just one string.


```r
corpus_tokens &lt;- 
  corpus_tokens %&gt;% 
  mutate(word = 
           str_replace_all(word,
                           "pandemic|covid|coronavirus",
                           "covid-19"))
```

---
class: middle

When we finally re-run the top-20 analysis, here is what we get.

---

.pull-left[
&lt;table class=" lightable-paper" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; rank &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; word &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; covid-19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 53 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; times &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 37 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; people &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; masks &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 33 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; mask &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; news &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; president &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; bannon &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; biden &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 23 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; black &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

.pull-right[

&lt;table class=" lightable-paper" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; rank &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; word &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; school &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; putin &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; u.s &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; child &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 15 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; children &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; percent &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 17 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; ukraine &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; costello &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; day &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 20 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; workers &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---

class: middle, center, inverse

# Word clouds

---

## Nobody loves word clouds

.pull-left[
Word clouds have an exceptionally bad reputation among NLP practitioners, digital text analysts, data scientists, computational linguists, etc. 

Here are some quotes from data-sciencey articles about word clouds:
]

.pull-right[

- "Word clouds are the pie chart of text data"
- "Word clouds considered harmful"
- "Word clouds are the mullets of the internet"

.center[&lt;img src="mullet.jpg" width="280px" /&gt;]

]

---
class: middle, center

## I don't really have a counterargument, but...

---

## Let's make one anyway

We will need to p_load() three packages.


```r
p_load(wordcloud, wordcloud2, RColorBrewer)
```

Wordclouds are made from exactly the kind of frequency counts that we've produced above: one column of words and another column of frequencies. Only our top-20 would not be quite enough words to fill a "cloud". 

---
## Create data object to draw the cloud from

.pull-left[
We'll run code similar to the one above, counting word frequencies, but retain the top-80 instead of the top-20 terms.


```r
top80 &lt;- 
  corpus_tokens %&gt;% 
  count(word, sort = T) %&gt;% 
  slice(1:80)
```
]

.pull-right[

```
## # A tibble: 80 × 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 covid-19     53
##  2 times        37
##  3 people       35
##  4 masks        33
##  5 mask         32
##  6 news         27
##  7 president    27
##  8 bannon       23
##  9 biden        23
## 10 black        22
## # … with 70 more rows
```
]

---
class: middle, center

## Word cloud code

---
class: middle

.pull-left[

```r
set.seed(1234) # for reproducibility 

wordcloud(words = top80$word, 
          freq = top80$n, 
          min.freq = 1,           
          max.words=80, 
          random.order=FALSE, 
          rot.per=0.35,            
          colors=brewer.pal(8, "Dark2"))
```
]

.pull-right[
&lt;img src="index_files/figure-html/unnamed-chunk-14-1.png" width="100%" /&gt;

]

---
class: middle, center

## Alternative: bring in shapes&lt;br /&gt;with {wordcloud2}

---
class: middle

.pull-left[


```r
wordcloud2(data = top80, 
           size = 0.7, 
           shape = 'heart')
```

]

.pull-right[

<div id="htmlwidget-9291966aef1e05064703" style="width:100%;height:252px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-9291966aef1e05064703">{"x":{"word":["covid-19","times","people","masks","mask","news","president","bannon","biden","black","school","putin","u.s","child","children","percent","ukraine","costello","day","workers","bannon's","russia","york","palin","trump","virginia","zucker","cbs","court","cuomo","don't","executive","records","students","vaccinated","19","cnn","health","kids","masking","defense","house","office","war","attorney","chris","macron","post","schools","told","washington","week","government","including","parents","prosecutors","question","team","caucus","clarke","costello's","didn't","giuliani","governor","justice","meeting","move","network","united","wearing","2021","continue","cruise","i'm","isn't","mandates","month","political","public","top"],"freq":[53,37,35,33,32,27,27,23,23,22,22,21,21,20,20,19,19,18,18,18,17,17,17,16,16,16,16,15,15,15,15,15,15,15,15,14,14,14,14,14,13,13,13,13,12,12,12,12,12,12,12,12,11,11,11,11,11,11,10,10,10,10,10,10,10,10,10,10,10,10,9,9,9,9,9,9,9,9,9,9],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":2.37735849056604,"backgroundColor":"white","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":true,"rotateRatio":0.4,"shape":"pentagon","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>

]

.center[&lt;br /&gt;Doesn't work &amp;mdash; needs more than 80 terms!
]


---
class: middle, center, inverse

# KWIC

---

## KWIC: Keywords in context

A time-honored way of approaching text corpora with computers. &lt;br /&gt;&lt;br /&gt;

--

.center[
&lt;img src="kwic_ac.jpeg" width="50%" /&gt;
]

---
## KWIC: Keywords in context

Generations of undergraduates have searched corpora with this method using software like WordSmith or AntConc.

.center[
&lt;img src="kwic_ac.jpeg" width="50%" /&gt;
]

---

## Let's do it with R!

.pull-left[There is a lovingly made R-package called {quanteda} that makes many of the established corpus methods available in R. So let's load it and do a KWIC search.
]

.pull-right[

```r
p_load(quanteda)
```

If you want to work with {quanteda}, your corpus has to be in the package's own "corpus" format. 

The function to convert it is quanteda::corpus().

]

---
class: middle

.pull-left[

```r
corpus &lt;- 
  corpus %&gt;% 
* group_by(doc_id) %&gt;%
* summarise(text =
*             paste0(text,
*                    collapse = " ")) %&gt;%
  corpus(docnames = id)

corpus %&gt;% 
  class()
```

```
## [1] "corpus"    "character"
```
]

--

.pull-right[

Before we can convert to the {quanteda} corpus format, we have to change the shape of the corpus one more time: {quanteda} expects a data frame in which each document name occurs only once. 

So we can't have one row per paragraph &amp;mdash; we must have one row per document.

]




---
class: middle

.pull-left[

```r
corpus     %&gt;% 
  tokens   %&gt;% 
* kwic(pattern = "child*",
       valuetype = "glob",  
       window = 3)
```
]

--

.pull-right[
The search term uses a wildcard. So strings will be retrieved even if they are longer than the explicit search pattern 
*child.*
]

---
class: middle

.pull-left[

```r
corpus     %&gt;% 
  tokens   %&gt;% 
  kwic(pattern = "child*",  
*      valuetype = "glob",
       window = 3)
```
]

--

.pull-right[
The argument `valuetype` must clarify the kind of search pattern we have entered. Possible values: `glob, regex, fixed`.
]

---
class: middle

.pull-left[

```r
corpus     %&gt;% 
  tokens   %&gt;% 
  kwic(pattern = "child*",  
       valuetype = "glob",  
*      window = 3)
```
]

--

.pull-right[
The argument `window` states how many words to the left and right of the target item we want to be returned.
]

---
background-image: url("kwic.png")
background-size: contain

---
class: inverse, middle, center

# The End

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
